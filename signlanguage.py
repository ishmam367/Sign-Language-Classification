# -*- coding: utf-8 -*-
"""SignLanguage.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17IHQBotYmhJPoI9AQA0J9iUxk7VJAQpD
"""

import matplotlib.pyplot as plt
import numpy as np 
import pandas as pd
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix

train_df= pd.read_csv("sign_mnist_train.csv")
test_df=pd.read_csv("sign_mnist_test.csv")

train_label= train_df['label']
X_train= train_df.drop(['label'],axis=1)
X_train= X_train.values.reshape(-1,28,28,1)
print(X_train.shape)

test_label= test_df['label']
X_test= test_df.drop(['label'],axis=1)
X_test=X_test.values.reshape(-1,28,28,1)
print(X_test.shape)

#Converting integers to binary form
from sklearn.preprocessing import LabelBinarizer 
lb = LabelBinarizer()

y_train=lb.fit_transform(train_label)
y_test=lb.fit_transform(test_label)

# Normalizing the data
X_train = X_train / 255
X_test = X_test / 255

f, ax = plt.subplots(2,5) 
f.set_size_inches(10, 10)
k = 0
for i in range(2):
    for j in range(5):
        ax[i,j].imshow(X_train[k].reshape(28,28), cmap = "gray")
        k += 1
    plt.tight_layout()

#Data augmentation
train_datagen = ImageDataGenerator(rotation_range = 0,
                                  height_shift_range=0.1,
                                  width_shift_range=0.1,
                                  shear_range=0,
                                  zoom_range=0.1,
                                  horizontal_flip=False,
                                  vertical_flip=False,
                                  fill_mode='nearest')

import tensorflow as tf
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.99):
      print("\nReached 99% validation accuracy so cancelling training!")
      self.model.stop_training = True
callbacks = myCallback()

#building Model
model = Sequential()
model.add(Conv2D(128 , (5,5) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(MaxPool2D((3,3) , strides = 2 , padding = 'same'))
model.add(Conv2D(64 , (2,2) , strides = 1 , padding = 'same' , activation = 'relu'))
#model.add(Dropout(0.2))
#model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Conv2D(32 , (2,2) , strides = 1 , padding = 'same' , activation = 'relu'))
#model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Flatten())
model.add(Dense(units=512,activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(units=24, activation= 'softmax'))
model.summary()

#compile and fitting the model
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
history=model.fit(train_datagen.flow(X_train,y_train,batch_size=200),
         epochs = 25,
         validation_data=(X_test,y_test),
         shuffle=1,
         callbacks=[callbacks]
         )

model.save("UpdatedSignModel.h5")

epochs = [i for i in range(7)]
fig , ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']
fig.set_size_inches(16,9)

ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')
ax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')
ax[0].set_title('Training & Validation Accuracy')
ax[0].legend()
ax[0].set_xlabel("Epochs")
ax[0].set_ylabel("Accuracy")

ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')
ax[1].plot(epochs , val_loss , 'r-o' , label = 'Testing Loss')
ax[1].set_title('Testing Accuracy & Loss')
ax[1].legend()
ax[1].set_xlabel("Epochs")
ax[1].set_ylabel("Loss")
plt.savefig('Accuracy and loss')
plt.show()